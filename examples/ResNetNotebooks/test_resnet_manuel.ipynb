{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'N': 1,\n",
    "    'seed': 0,\n",
    "    'density': 500,\n",
    "    'h': 1.0,\n",
    "    'l2_regularisation': 0.000025,\n",
    "    'bias_mean_initialisation': False,\n",
    "    'batch_normalization': False,\n",
    "    \n",
    "    'resnet_block_kwargs': {\n",
    "        'filters': [32, 32],\n",
    "        'kernel_size': [100, 100],\n",
    "        'padding': 'same',\n",
    "        'activation': 'softplus',\n",
    "        'add_input': True\n",
    "    },\n",
    "    \n",
    "    'final_block_kwargs': {\n",
    "        'filters': [1],\n",
    "        'kernel_size': [100],\n",
    "        'padding': 'same',\n",
    "        'activation': None,\n",
    "        'add_input': False\n",
    "    },\n",
    "    \n",
    "    'blocks_length': 4,\n",
    "    \n",
    "    'metrics': {\n",
    "        'kinetic_energy': 'mean_absolute_error',\n",
    "        'kinetic_energy_density': 'mean_absolute_error',\n",
    "        'derivative': 'mean_absolute_error'\n",
    "    },\n",
    "    \n",
    "    'loss': {\n",
    "        'kinetic_energy': 'mean_squared_error',\n",
    "        'kinetic_energy_density': 'mean_squared_error',\n",
    "        'derivative': 'mean_squared_error'\n",
    "    },\n",
    "    \n",
    "    'loss_weights': {\n",
    "        'kinetic_energy': 1.0,\n",
    "        'kinetic_energy_density': 1.0,\n",
    "        'derivative': 1.0\n",
    "    },\n",
    "    \n",
    "    'optimizer_kwargs': {\n",
    "        'learning_rate': 'WarmupExponentialDecay',\n",
    "        'beta_1': 0.9,\n",
    "        'beta_2': 0.999,\n",
    "        'epsilon': 1e-07,\n",
    "        'amsgrad': False,\n",
    "        'clipnorm': 100.0,\n",
    "\n",
    "        'learning_rate_kwargs': {\n",
    "            'initial_learning_rate': 0.0001,\n",
    "            'final_learning_rate': 0.0000000001,\n",
    "            'decay_rate': 0.9,\n",
    "            'decay_steps': 2000, # batches\n",
    "\n",
    "            'cold_steps': 43700, # batches\n",
    "            'warmup_steps': 0, # batches\n",
    "            'cold_factor': 1.0\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'fit_kwargs': {\n",
    "        'batch_size': 100,\n",
    "        'epochs': 20000,\n",
    "        'verbose': 0,\n",
    "        #validation_split: 0.1\n",
    "        'validation_freq': 1000,\n",
    "        'shuffle': True,\n",
    "        'initial_epoch': 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# from https://github.com/hauser-group/odft_tools\n",
    "from odft_tools.layers import (\n",
    "    IntegrateLayer,\n",
    "    Continuous1DConvV1,\n",
    "    Continuous1DConvV2\n",
    ")\n",
    "\n",
    "from odft_tools.models import (\n",
    "    ResNetContConv1DModel,\n",
    "    ResNetConv1DModel,\n",
    "    ResNetCheckModel\n",
    ")\n",
    "\n",
    "data_path = '../datasets/orbital_free_DFT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(data_path + 'M=100_training_data.hdf5', 'r') as f:\n",
    "    keys = f.keys()\n",
    "    print(keys)\n",
    "    # build a dict (dataset.value has been deprecated. Use dataset[()] instead.)\n",
    "    data = {key:f[key][()] for key in keys}\n",
    "\n",
    "x = np.linspace(0, 1, 500)\n",
    "dx = x[1] - x[0]\n",
    "N = 1\n",
    "# density is wavefunction squared\n",
    "n = np.sum(data['wavefunctions'][:, :, :N]**2, axis=-1)\n",
    "# integrate using trapezoidal rule:\n",
    "V = np.sum(0.5*(data['potential'][:, :-1]*n[:, :-1] \n",
    "                + data['potential'][:, 1:]*n[:, 1:])           \n",
    "           * dx, axis=-1)\n",
    "# kinetic energy is total energy minus potential energy\n",
    "T = np.sum(data['energies'][:, :N], axis=-1) - V\n",
    "# kinetic energy derivative\n",
    "dT_dn = np.expand_dims(np.sum(data['energies'][:, :N], axis=-1)/N, axis=-1) - data['potential']\n",
    "n = n.reshape((-1, 500, 1))\n",
    "\n",
    "with h5py.File(data_path + 'test_data.hdf5', 'r') as f:\n",
    "    keys = f.keys()\n",
    "    print(keys)\n",
    "    # build a dict (dataset.value has been deprecated. Use dataset[()] instead.)\n",
    "    data_test = {key:f[key][()] for key in keys}\n",
    "    \n",
    "# density is wavefunction squared\n",
    "n_test = np.sum(data_test['wavefunctions'][:, :, :N]**2, axis=-1)\n",
    "# integrate using trapezoidal rule:\n",
    "V_test = np.sum(0.5*(data_test['potential'][:, :-1]*n_test[:, :-1]\n",
    "                + data_test['potential'][:, 1:]*n_test[:, 1:])\n",
    "                * dx, axis=-1)\n",
    "# kinetic energy is total energy minus potential energy\n",
    "T_test = np.sum(data_test['energies'][:, :N], axis=-1) - V_test\n",
    "# kinetic energy derivative\n",
    "dT_dn_test = - data_test['potential'] + np.expand_dims(np.sum(data_test['energies'][:, :N], axis=-1)/N, axis=-1) \n",
    "n_test = n_test.reshape((-1, 500, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet_KineticEnergyDensityFunctional(params):\n",
    "    kernel_regularizer = tf.keras.regularizers.l2(params['l2_regularisation']) if params.get('l2_regularisation', 0.0) > 0.0 else None\n",
    "    bias_initializer = tf.constant_initializer(value=params['dataset']['targets_mean']['kinetic_energy']) if params.get('bias_mean_initialisation', False) else None\n",
    "\n",
    "    density = tf.keras.layers.Input(shape=params['density'], name='density')\n",
    "    value = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(density)\n",
    "    \n",
    "    def resnet_block(input, filters, kernel_size, padding=None, activation=None, add_input=True):\n",
    "        value = input\n",
    "        for layer in range(len(filters)):\n",
    "            value = tf.keras.layers.Conv1D(filters=filters[layer], \n",
    "                                        kernel_size=kernel_size[layer], \n",
    "                                        padding=padding,\n",
    "                                        use_bias=True,\n",
    "                                        activation= activation if layer < len(filters)-1 else None,\n",
    "                                        kernel_regularizer=kernel_regularizer)(value)\n",
    "\n",
    "        if add_input:\n",
    "            value = tf.keras.layers.Add()([value, input])\n",
    "        \n",
    "        if activation is not None:\n",
    "            return tf.keras.layers.Activation(activation=activation)(value)\n",
    "        else:\n",
    "            return value\n",
    "\n",
    "    for layer in range(params['blocks_length']):\n",
    "        if layer < params['blocks_length'] - 1:\n",
    "            value = resnet_block(value, **params['resnet_block_kwargs'])\n",
    "        else:\n",
    "            value = resnet_block(value, **params['final_block_kwargs'])\n",
    "            \n",
    "    kinetic_energy_density = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=-1), name='kinetic_energy_density')(value)\n",
    "    kinetic_energy = IntegrateLayer(params['h'], name='kinetic_energy')(kinetic_energy_density)\n",
    "    return tf.keras.Model(inputs={'density': density}, outputs={'kinetic_energy': kinetic_energy, 'kinetic_energy_density': kinetic_energy_density})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices((n.astype(np.float32), {'T': T.astype(np.float32), 'dT_dn': dT_dn.astype(np.float32)})).batch(100).repeat(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetCheckModel(params)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), \n",
    "              loss={'T': 'mse', 'dT_dn': 'mse'}, \n",
    "              loss_weights={'T': 0.2, 'dT_dn': 1.0}, # As recommended by Manuel: scale the loss in T by 0.2\n",
    "              metrics={'T': ['mae'], 'dT_dn': ['mae']})\n",
    "model.build(input_shape=(None, 500, 1))\n",
    "# model.summary()\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((n.astype(np.float32), {'T': T.astype(np.float32), 'dT_dn': dT_dn.astype(np.float32)})).batch(100).repeat(10)\n",
    "model.fit(training_dataset, epochs=50, verbose=0, validation_data=(n_test, {'T': T_test, 'dT_dn': dT_dn_test}), validation_freq=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
