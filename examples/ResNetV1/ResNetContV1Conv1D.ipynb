{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# from https://github.com/hauser-group/odft_tools\n",
    "from odft_tools.layers import (\n",
    "    IntegrateLayer,\n",
    "    Continuous1DConvV1\n",
    ")\n",
    "\n",
    "from odft_tools.models_resnet_ccn import (\n",
    "    ResNetContConv1DModel,\n",
    "    ResNetContConv1DV2Model,\n",
    "    ResNetConv1DModel\n",
    ")\n",
    "\n",
    "from odft_tools.utils import (\n",
    "    plot_derivative_energy,\n",
    "    plot_gaussian_weights_v1\n",
    ")\n",
    "\n",
    "from odft_tools.keras_utils import (\n",
    "    WarmupExponentialDecay\n",
    ")\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "import random\n",
    "\n",
    "data_path = '../datasets/orbital_free_DFT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(data_path + 'dataset_large.hdf5', 'r') as f:\n",
    "    keys = f.keys()\n",
    "    print(keys)\n",
    "    # build a dict (dataset.value has been deprecated. Use dataset[()] instead.)\n",
    "    data = {key:f[key][()] for key in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 500)\n",
    "dx = x[1] - x[0]\n",
    "N = 1\n",
    "# density is wavefunction squared\n",
    "n = np.sum(data['wavefunctions'][:, :, :N]**2, axis=-1)\n",
    "# integrate using trapezoidal rule:\n",
    "V = np.sum(0.5*(data['potential'][:, :-1]*n[:, :-1] \n",
    "                + data['potential'][:, 1:]*n[:, 1:])           \n",
    "           * dx, axis=-1)\n",
    "# kinetic energy is total energy minus potential energy\n",
    "T = np.sum(data['energies'][:, :N], axis=-1) - V\n",
    "# kinetic energy derivative\n",
    "dT_dn = np.expand_dims(np.sum(data['energies'][:, :N], axis=-1)/N, axis=-1) - data['potential']\n",
    "n = n.reshape((-1, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(data_path + 'dataset_validate.hdf5', 'r') as f:\n",
    "    keys = f.keys()\n",
    "    print(keys)\n",
    "    # build a dict (dataset.value has been deprecated. Use dataset[()] instead.)\n",
    "    data_test = {key:f[key][()] for key in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density is wavefunction squared\n",
    "n_test = np.sum(data_test['wavefunctions'][:, :, :N]**2, axis=-1)\n",
    "# integrate using trapezoidal rule:\n",
    "V_test = np.sum(0.5*(data_test['potential'][:, :-1]*n_test[:, :-1] \n",
    "                + data_test['potential'][:, 1:]*n_test[:, 1:])           \n",
    "                * dx, axis=-1)\n",
    "# kinetic energy is total energy minus potential energy\n",
    "T_test = np.sum(data_test['energies'][:, :N], axis=-1) - V_test\n",
    "# kinetic energy derivative\n",
    "dT_dn_test = - data_test['potential'] + np.expand_dims(np.sum(data_test['energies'][:, :N], axis=-1)/N, axis=-1) \n",
    "n_test = n_test.reshape((-1, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 100\n",
    "mean = 5\n",
    "stddev = 5\n",
    "\n",
    "num_res_net_blocks = 6\n",
    "epoch = 2000\n",
    "\n",
    "density = {'n': n.astype(np.float32)}\n",
    "targetdata = {'T': T.astype(np.float32), 'dT_dn': dT_dn.astype(np.float32)}\n",
    "\n",
    "# training_dataset = tf.data.Dataset.from_tensor_slices((n.astype(np.float32), {'T': T.astype(np.float32), 'dT_dn': dT_dn.astype(np.float32)})).batch(100).repeat(10)\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "decay_steps = 2000\n",
    "decay_rate= 0.9\n",
    "\n",
    "initial_learning_rate = WarmupExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=False,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "path = '/ResNetContConv1D/'\n",
    "\n",
    "\n",
    "seed = 0\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=50,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model = ResNetContConv1DModel(\n",
    "    filter_size=32,\n",
    "    kernel_size=100,\n",
    "    layer_size=None,\n",
    "    num_res_net_blocks=num_res_net_blocks,\n",
    "    weights_gaus=[5, 5],\n",
    "    n_outputs=None,\n",
    "    random_init=True,\n",
    "    dx=0.002\n",
    ")\n",
    "\n",
    "model.create_res_net_model()\n",
    "model.build(input_shape=(1, 500))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate, amsgrad=False), \n",
    "              loss={'T': 'mse', 'dT_dn': 'mse'}, \n",
    "              loss_weights={'T': 0.2, 'dT_dn': 1.0}, # As recommended by Manuel: scale the loss in T by 0.2\n",
    "              metrics={'T': ['mae'], 'dT_dn': ['mae']})\n",
    "print('--------------------------------->Start<---------------------------------')\n",
    "print(f'No Cont Layer. res_net {num_res_net_blocks} with {epoch}')\n",
    "\n",
    "model.models.summary()\n",
    "weights_before_train = model.layers[0].get_weights()[0]\n",
    "model.fit(x=density, y=targetdata, epochs=epoch, verbose=2, validation_data=(n_test, {'T': T_test, 'dT_dn': dT_dn_test}), validation_freq=10, callbacks=[callback])\n",
    "weights_after_train = model.layers[0].get_weights()[0]\n",
    "print('--------------------------------->END<---------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gaussian_weights_v1(weights_before_train, ' before', path)\n",
    "plot_gaussian_weights_v1(weights_after_train, ' after', path)\n",
    "plot_derivative_energy(x, dT_dn, model, n, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([])\n",
    "df['loss'] = model.history.history['loss']\n",
    "df['dT_dn_loss'] = model.history.history['dT_dn_loss']\n",
    "df['T_loss'] = model.history.history['T_loss']\n",
    "\n",
    "df.to_csv('results' + path + '/losses.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 3))\n",
    "\n",
    "plt.plot(df['loss'][1:])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss a.u.')\n",
    "plt.title('loss over epochs for ResNet CCNN')\n",
    "plt.savefig('results/' + path + 'loss_ResNet_CNNV1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
