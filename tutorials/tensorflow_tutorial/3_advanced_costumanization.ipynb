{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c3117a-f2db-4722-b91a-01f66b218255",
   "metadata": {},
   "source": [
    "# Advanced Customization\n",
    "\n",
    "Here we will build a one-dimensional convolutional neural network. We will customize the model, layer and kernel class of Tensorflow/Keras.\n",
    "\n",
    "A layer with Gaussian kernel method is going to be implemented. Actually, kernels are mostly random normally distributed,\n",
    "but we want a kernel function that is smoother and vanishing on the left and right hand side, but still random in some sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2ce34-c51f-434e-b991-631856b0ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the needed packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils.utils import (\n",
    "    calc_gaussians,\n",
    "    gen_gaussian_kernel_v1_1D,\n",
    "    load_data_cnn,\n",
    "    plot_gaussian_weights,\n",
    "    plot_derivative_energy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa4eafd-861e-4023-90bf-e41aac47d6ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data\n",
    "\n",
    "A one-dimensional model system of noninteracting spinless fermions with one particle in a hard wall box and a \n",
    "potential described by a linear combination of three Gaussians has been investigated. \n",
    "With the Numerov’s method the 1D Schrödinger equation for these potentials is solved\n",
    "on a grid of $G = 500$ points. The solutions are then used to compute the data for the training.\n",
    "\n",
    "The input data for the neural network is going to be the density with the dimension (500, 1) and the target data is going to be the kinetic energy with the dimension (1) and the kintetics energy derivative with the dimension (500). All of the data have a length of 100. <br>\n",
    "The difference between the dimension of density and the kintetics energy derivative, is that the density list of lists ( [ [value], [value]. ...] ) and kintetics energy derivative is a list with a lenght of 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd10319-29ac-4342-923e-06fb50db2c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/orbital_free_DFT/'\n",
    "\n",
    "kinetic_train, kinetic_derivativ_train, density_train = load_data_cnn(\n",
    "    path=data_path,\n",
    "    data_name='M=100_training_data.hdf5'\n",
    ")\n",
    "\n",
    "kinetic_test, kinetic_derivativ_test, density_test = load_data_cnn(\n",
    "    path=data_path,\n",
    "    data_name='test_data.hdf5'\n",
    ")\n",
    "print(f'Dimenstion of density data {np.shape(density_train)}')\n",
    "print(f'Dimenstion of kinetic derivative data {np.shape(kinetic_derivativ_train)}')\n",
    "print(f'Dimenstion of kinetic energy data {np.shape(kinetic_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afa2f5-849c-419c-9e21-7a06096970f3",
   "metadata": {},
   "source": [
    "## Custom Kernel\n",
    "\n",
    "The Kernel class describes/returns how the kernel values are set.\n",
    "We are overwirtting the __init__ and the call method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01946ab9-c890-4584-8961-106a77942cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the packages which are needed for the class.\n",
    "from tensorflow.python.ops.init_ops_v2 import Initializer\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops.init_ops_v2 import _assert_float_dtype\n",
    "\n",
    "\n",
    "class ContinuousConvKernel1DV1(Initializer):\n",
    "    '''\n",
    "     weights_init: 2 dimensianl array. First entry is mean, secound is std\n",
    "     create_continuous_kernel: method that creates the values for a continuous kernel\n",
    "     random_init: Bool if the initialization is random \n",
    "     seed: integer if radom values should be the same.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 weights_init,\n",
    "                 create_continuous_kernel=None,\n",
    "                 random_init=False,\n",
    "                 seed=None):\n",
    "        \n",
    "        if not create_continuous_kernel:\n",
    "            raise ValueError(\"Set a continuous kernel\")\n",
    "\n",
    "        # We overwritte the init method. We are checkking if weights_init\n",
    "        # consists of two variables (mean and std)\n",
    "        if len(weights_init) != 2:\n",
    "            raise ValueError(\"weights_init length must be 2\")\n",
    "        # Checking if mean is greater zero\n",
    "        if weights_init[0] < 0:\n",
    "            raise ValueError(\"'mean' must be positive float\")\n",
    "        # Checking if std is greater zero\n",
    "        if weights_init[1] < 0:\n",
    "            raise ValueError(\"'stddev' must be positive float\")\n",
    "\n",
    "        self.weights_init = weights_init\n",
    "        self.random_init = random_init\n",
    "        self.create_continuous_kernel = create_continuous_kernel\n",
    "\n",
    "\n",
    "    def __call__(self, shape, dtype=dtypes.float32):\n",
    "        # We are overwritting the call method and setting the gaussian kernel\n",
    "        \"\"\"Returns a tensor object initialized as specified by the initializer.\n",
    "        Args:\n",
    "          shape: Shape of the tensor.\n",
    "          dtype: Optional dtype of the tensor. Only floating point types are\n",
    "              supported.\n",
    "        Raises:\n",
    "          ValueError: If the dtype is not floating point\n",
    "        \"\"\"\n",
    "\n",
    "        dtype = _assert_float_dtype(dtype)\n",
    "\n",
    "        continuous_kernel = self.create_continuous_kernel(\n",
    "            shape=shape,\n",
    "            weights=self.weights_init,\n",
    "            dtype=dtype,\n",
    "            random_init=self.random_init)\n",
    "        # The custom kernel is returned\n",
    "        return continuous_kernel\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"mean\": self.weights_init[0],\n",
    "            \"stddev\": self.weights_init[1],\n",
    "            \"raondom_init\": self.random_init\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf2604-f3ac-4906-9a82-f80f7353e880",
   "metadata": {},
   "source": [
    "## Custom Layers\n",
    "\n",
    "We need two custom layers. One Layer with the custom kernel initilaizer and one layer that integrates its input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2594c8-6600-4dde-8d11-a2157e2d4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousConv1D(keras.layers.Conv1D):\n",
    "    '''\n",
    "    Here, we just overwritte of the kernel_initializer in the _init_ method.\n",
    "    The method for creating the custom kernel is passed to the __init__ as a parameter \n",
    "    and can be found in utils/utils.py\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 weights_init,\n",
    "                 create_continuous_kernel,\n",
    "                 random_init=False,\n",
    "                 seed=None,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Set custom kernel init. with gaussian kernel\n",
    "        self.kernel_initializer = ContinuousConvKernel1DV1(\n",
    "            weights_init=weights_init,\n",
    "            create_continuous_kernel=create_continuous_kernel,\n",
    "            random_init=random_init,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "class IntegrateLayer(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    In the call method we implement the trapezoidal integral and in the init we pass h for inte\n",
    "    '''\n",
    "    def __init__(self, h=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = h\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.h * tf.reduce_sum(\n",
    "            (inputs[:, :-1] + inputs[:, 1:]) / 2.,\n",
    "            axis=1, name='trapezoidal_integral_approx')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'h': self.h})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43919552-e487-451d-8852-b28cda412a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNNV1Model(keras.Model):\n",
    "    '''\n",
    "    filter_size: integer. How many filters should be created in a layer.\n",
    "    kernel_sice: integer. How many kernel functions are in one filter\n",
    "    layer_length: integer. Layer length\n",
    "    create_continuous_kernel: method that creates the values for a continuous kernel\n",
    "    kernel_regularizer: float. kernel regulizer\n",
    "    dx: float. integration step\n",
    "    weights: 2 dimensianl array. First entry is mean, secound is std\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            filter_size,\n",
    "            kernel_size,\n",
    "            layer_length,\n",
    "            create_continuous_kernel,\n",
    "            kernel_regularizer,\n",
    "            activation,\n",
    "            dx=0.002,\n",
    "            weights=[5, 5]):\n",
    "        super().__init__()\n",
    "        self.dx = dx\n",
    "        self.conv_layers = []\n",
    "        mean = weights[0]\n",
    "        stddev = weights[1]\n",
    "        \n",
    "        # Here we create continuous kernels in a foor loop\n",
    "        for l in range(layer_length):\n",
    "            cont_layer = ContinuousConv1D(\n",
    "                filters=filter_size,\n",
    "                kernel_size=kernel_size,\n",
    "                padding='same',\n",
    "                weights_init=[mean, stddev],\n",
    "                create_continuous_kernel=create_continuous_kernel,\n",
    "                kernel_regularizer=kernel_regularizer,\n",
    "                activation=activation,\n",
    "                random_init=True,\n",
    "            )\n",
    "            self.conv_layers.append(cont_layer)\n",
    "        \n",
    "        # last layer is fixed to use a single filter\n",
    "        cont_layer = ContinuousConv1D(\n",
    "            filters=1,\n",
    "            kernel_size=kernel_size,\n",
    "            padding='same',\n",
    "            weights_init=[mean, stddev],\n",
    "            create_continuous_kernel=create_continuous_kernel,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            activation=activation,\n",
    "            random_init=True\n",
    "        )\n",
    "        self.conv_layers.append(cont_layer)\n",
    "        self.integrate = IntegrateLayer(dx)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(inputs)\n",
    "            # Calculate kinetic energy density tau by applying convolutional layers\n",
    "            tau = inputs\n",
    "            for layer in self.conv_layers:\n",
    "                tau = layer(tau)\n",
    "            # Kinetic energy T is integral over kinetiv energy density\n",
    "            T = self.integrate(tau)\n",
    "        # The discretized derivative needs to be divided by dx\n",
    "        dT_dn = tape.gradient(T, inputs)/self.dx\n",
    "        return {'T': T, 'dT_dn': dT_dn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3a186-7112-40ae-ab2b-3ccf1cd7bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the training data in a tensor slice\n",
    "# Advantage: Batch size and how often it\n",
    "# should be reapeted can defined here.\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        density_train.astype(np.float32),\n",
    "        {'T': kinetic_train.astype(np.float32),\n",
    "        'dT_dn': kinetic_derivativ_train.astype(np.float32)}\n",
    "    )\n",
    ").batch(100).repeat(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed8d03-b11b-4fa7-b057-6f6f3af33e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 100\n",
    "layer_length = 3\n",
    "activation = 'softplus'\n",
    "epoch = 10\n",
    "\n",
    "# Now, we initialize/build/compile the model\n",
    "model = CustomCNNV1Model(\n",
    "    filter_size=32,\n",
    "    kernel_size=kernel_size,\n",
    "    layer_length=layer_length,\n",
    "    dx=0.002,\n",
    "    create_continuous_kernel=gen_gaussian_kernel_v1_1D,\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.00025),\n",
    "    activation=activation\n",
    ")\n",
    "\n",
    "\n",
    "model.build(input_shape=(None, 500, 1))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.0001, amsgrad=False\n",
    "    ),\n",
    "    loss={'T': 'mse', 'dT_dn': 'mse'},\n",
    "    loss_weights={'T': 0.2, 'dT_dn': 1.0}, # scale the loss in T by 0.2\n",
    "    metrics={'T': ['mae'], 'dT_dn': ['mae']}\n",
    ")\n",
    "\n",
    "weights_before_train = model.layers[0].get_weights()[0]\n",
    "\n",
    "# Let's print the summary of our model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7247c-ee5d-475c-bd92-db4bab296d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip: with tf.device('/device:GPU:0'): you can run/train the machine learning model on a specific GPU\n",
    "'''\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model.fit(....\n",
    "'''\n",
    "\n",
    "model.fit(\n",
    "    training_dataset,\n",
    "    epochs=epoch,\n",
    "    verbose=2, # With 0, there no print of loss. With 1, the print is with loss and a loading bar (batch/epoch), with 2 print with loss \n",
    "    validation_data=(\n",
    "        density_test, {'T': kinetic_test, 'dT_dn': kinetic_derivativ_test}\n",
    "    ),\n",
    "    validation_freq=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf790d3-15a8-47ae-b983-73665d12e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_after_train = model.layers[0].get_weights()[0]\n",
    "# Plot of 32 kernels with a length of 100, before and after the training\n",
    "plot_gaussian_weights(weights_before_train, 'before')\n",
    "plot_gaussian_weights(weights_after_train, 'after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9da95c-703a-45fa-bcdc-2d4de9da6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 500)\n",
    "# Plotting the energy derivative. Reference and trained\n",
    "plot_derivative_energy(x, kinetic_derivativ_train, model, density_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
